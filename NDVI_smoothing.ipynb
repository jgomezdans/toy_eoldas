{
 "metadata": {
  "name": "NDVI_smoothing"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "%pylab inline\n",
      "plt.rcParams['figure.dpi'] = 2*plt.rcParams['figure.dpi']\n",
      "plt.rcParams['savefig.dpi'] = 2 * plt.rcParams['savefig.dpi']\n",
      "plt.rcParams['text.usetex'] = True"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Welcome to pylab, a matplotlib-based Python environment [backend: module://IPython.zmq.pylab.backend_inline].\n",
        "For more information, type 'help(pylab)'.\n"
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Experiments smoothing univariate time series"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "J G\u00f3mez-Dans (NCEO & UCL)\n",
      "j.gomez-dans@ucl.ac.uk"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The fundamental problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The fundamental problem we are trying to solve with EO-LDAS (and indeed most other inversion approaches) is to *infer the state of the land surface, given all available observations, as well as any other relevant information*. This leads quite naturally to Bayes' Rule, that states\n",
      "\n",
      "$$\n",
      "p(\\theta|y) \\propto p(\\theta)\\cdot p(y|\\theta)\n",
      "$$\n",
      "\n",
      "In other words, the *distribution* of $\\theta$ (the variable of interest) **conditioned** by $y$ (the observations or evidence) is proportional to the product of whatever information we might have had of $\\theta$ to start with (from previous experiments, expert knowledge, etc.) times the probability of the observations given the true parameters (the likelihood function)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Defining the prior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The prior distribution is an assessment of what we know before we even start the experiments. Typically, it can include realistic parameter boundaries (for example, weight cannot be less than 0!), typical values, etc. A common approach is to use a Gaussian distribution, controlled by two parameters: the mean($\\mu_p$) and the standard deviation ($\\sigma_p$). $\\mu_p$ controls the typical location of the magnitude, whereas $\\sigma_p$ informs us of how certain/uncertain we are about it. A Gaussian pdf is\n",
      "$$\n",
      "p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_P}\\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_p^2}\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gauss ( x, mu, sigma ):\n",
      "    c1 = 1./(np.sqrt ( 2.*np.pi)*sigma)\n",
      "    c2 = -0.5*( x - mu )**2/sigma**2\n",
      "    return c1*np.exp(c2)\n",
      "x = np.arange(-10,10,0.02)\n",
      "plt.plot ( x, gauss(x,0, 1), '-r', label=r'$\\sigma=1$')\n",
      "plt.plot ( x, gauss(x,0, 2), '-g', label=r'$\\sigma=2$')\n",
      "plt.plot ( x, gauss(x,0, 10), '-b', label=r'$\\sigma=10$')\n",
      "plt.plot ( x, gauss(x,0, 0.5), '-k', label=r'$\\sigma=0.5$')\n",
      "plt.plot ( x, gauss(x,0, 0.025), '-y', label=r'$\\sigma=0.025$')\n",
      "plt.ylim(0,2)\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 213,
       "text": [
        "<matplotlib.legend.Legend at 0x21424850>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x736c990>"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(-10,10,0.02)\n",
      "plt.plot ( x, gauss(x,-5, 1), '-r', label=r'$\\sigma=1\\;\\mu=-5$')\n",
      "plt.plot ( x, gauss(x,0, 2), '-g', label=r'$\\sigma=2\\;\\mu=2$')\n",
      "plt.plot ( x, gauss(x,3, 10), '-b', label=r'$\\sigma=10\\;\\mu=10$')\n",
      "plt.ylim(0,0.5)\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 214,
       "text": [
        "<matplotlib.legend.Legend at 0x1fa146d0>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x20dd3590>"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The likelihood function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The likelihood function matches the parameter to the observations. Again we will use Gaussian distributions for this. The likelihood basically states the probability of measurement $y$ given a value of the parameter $x$. So if the mapping between $x$ and $y$ is $x=y$, the only difference is given by additive noise, with a Gaussian distribution:\n",
      "$$\n",
      "y = x + \\epsilon\n",
      "$$\n",
      "\n",
      "$$\n",
      "\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma_{obs})\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Combining prior and likelihood"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From Bayes' Theorem, we have that the *posterior distribution* of $x$ is the product of prior and likelihood:\n",
      "\n",
      "$$ \n",
      "p(x|y) = \\frac{1}{\\sqrt{2\\pi}\\sigma_P}\\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_p^2}\\right]\\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma_{obs}}\\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_{obs})^2}{\\sigma_{obs}^2}\\right]\n",
      "$$\n",
      "\n",
      "Ignoring the constants...\n",
      "$$ \n",
      "p(x|y) \\propto \\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_p^2}\\right]\\cdot \\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_{obs})^2}{\\sigma_{obs}^2}\\right]\n",
      "$$\n",
      "\n",
      "It is useful to work in terms of logarithms...\n",
      "$$\n",
      "\\log(p(x|y)) \\propto \\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_p^2}\\right] + \\left[-\\frac{1}{2}\\frac{(x-\\mu_{obs})^2}{\\sigma_{obs}^2}\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's assume that the prior distribution is centred at -5 with a standard deviation of 1, and that the likelihood is centered at 0 with a standard distribution of 2. What does this mean? It basically means that before we even do anything, we are quite sure that the value will be quite close to -5. It also means that the evidence (suggesting a value of 0 with a standard deviation of 2) is of little information content, compared to the prior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prior = gauss(x,-5, 1)\n",
      "likelihood = gauss(x,0, 2)\n",
      "posterior = prior * likelihood\n",
      "plt.subplot(2,1,1)\n",
      "plt.plot ( x, prior, '-r', label=\"Prior\")\n",
      "plt.plot ( x, likelihood, '-g', label=\"Likelihood\")\n",
      "plt.plot ( x, posterior, '-b', label=\"Posterior\")\n",
      "plt.axvline ( x[posterior.argmax()])\n",
      "plt.legend(loc='best')\n",
      "plt.subplot(2,1,2)\n",
      "plt.plot ( x, np.log(prior), '-r', label=\"Prior\")\n",
      "plt.plot ( x, np.log(likelihood), '-g', label=\"Likelihood\")\n",
      "plt.plot ( x,np.log(likelihood) + np.log(prior) , '-b', label=\"Posterior\")\n",
      "plt.axvline ( x[posterior.argmax()])\n",
      "plt.legend(loc='best')\n",
      "print \"Maximum a posteriori: \", x[posterior.argmax()]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Maximum a posteriori:  -4.0\n"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x1fa21550>"
       ]
      }
     ],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prior = gauss(x,-5, 5)\n",
      "likelihood = gauss(x,0, 1)\n",
      "posterior = prior * likelihood\n",
      "plt.subplot(2,1,1)\n",
      "plt.plot ( x, prior, '-r', label=\"Prior\")\n",
      "plt.plot ( x, likelihood, '-g', label=\"Likelihood\")\n",
      "plt.plot ( x, posterior, '-b', label=\"Posterior\")\n",
      "plt.axvline ( x[posterior.argmax()])\n",
      "plt.legend(loc='best')\n",
      "plt.subplot(2,1,2)\n",
      "plt.plot ( x, np.log(prior), '-r', label=\"Prior\")\n",
      "plt.plot ( x, np.log(likelihood), '-g', label=\"Likelihood\")\n",
      "plt.plot ( x,np.log(likelihood) + np.log(prior) , '-b', label=\"Posterior\")\n",
      "plt.axvline ( x[posterior.argmax()])\n",
      "plt.legend(loc='best')\n",
      "print \"Maximum a posteriori: \", x[posterior.argmax()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Maximum a posteriori:  -0.2\n"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x1f5ef910>"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the form of the posterior from the equation that defines the log posterior above"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Multivariate distributions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The previous examples were univariate. What if the problem you try to solve is multidimensional? The pdf equivalent for an $n-$dimensional problem is the multivariate Gaussian distribution. Here, we define the probability of a vector $\\mathbf{x}$ as being a function of a mean vector $\\mathbf{\\mu}$ and a *covariance matrix* $\\mathbf{C}$\n",
      "$$\n",
      "p(\\mathbf{x}) \\propto \\exp\\left[-\\frac{1}{2}\\left(\\mathbf{x} - \\mathbf{\\mu}\\right)^{T}\\mathbf{C}^{-1}\\left(\\mathbf{x} - \\mathbf{\\mu}\\right) \\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "DA & Smoothing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately, in most real life problems, the observations only have limited information content, and it is not possible to retrieve all the parameters that we want to find out about purely from analysing the observations. We tend to call these problems *ill-posed* (although this is a more general term). A way around this situation is to add extra constraints from prior information (See e.g. Combal B. et al. Retrieval of canopy biophysical variables from bidirectional reflectance: Using prior information to solve the ill-posed inverse problem //Remote sensing of environment. \u2013 2003. \u2013 \u0422. 84. \u2013 \u2116. 1. \u2013 \u0421. 1-15.). Apart from setting expected distributions of parameters, as we've seen above, we usually also know a lot about e.g. the temporal evolution of things. For example, leaves usually evolve smoothly. So a solution that is very jagged is less likely than one that is smooth. This is the reason one of the first tasks people do to use LAI data from satellites is smoothing. A better way is to enforce smoothness as we solve the problem.\n",
      "\n",
      "Smoothness is characterised by small derivatives (or discrete differences). So one constraint could be that the discrete differences of the parameter as it evolves in time are as small as possible, while still fitting the observations and any other prior laws. Equivalently, we are effectively stating that the evolution of the value of the parameter from one time step to the next should be as small as possible or, in other words: the value tomorrow should be the same as the value today, but with some added uncertainty:\n",
      "$$\n",
      "x_{t+1} = x_{t} + \\epsilon_{t}\n",
      "$$\n",
      "If $\\epsilon_t=0$, then $\\mathbf{x}=C$, a constant!, so we know that $\\epsilon_{t} > 0$. We can assume that $\\epsilon_{t} \\sim \\mathcal{N}(0, \\sigma_{m})$, so \n",
      "$$\n",
      "\\log(p(\\mathbf{x}))\\propto\\gamma \\left(\\Delta\\cdot\\mathbf{x}\\right)^{T}\\mathbf{I}\\left(\\Delta\\cdot\\mathbf{x}\\right)\n",
      "$$\n",
      "So by inspection, $\\gamma=\\sigma_{m}^{-2}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(1,366 )\n",
      "y_smooth = (1-np.cos(np.pi*x/366.)**2)\n",
      "y_noisy = y_smooth + np.random.randn(365)*0.15\n",
      "plt.plot(x,y_noisy, '-g', lw=0.5)\n",
      "plt.plot(x,y_smooth, '-r',lw=1.4)\n",
      "\n",
      "deriv_smooth = np.diff ( y_smooth )\n",
      "deriv_noisy = np.diff ( y_noisy )\n",
      "print \"RMS of derivative of smooth function:\", np.sqrt(np.mean(deriv_smooth**2))\n",
      "print \"RMS of derivative of noisy function:\", np.sqrt(np.mean(deriv_noisy**2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RMS of derivative of smooth function: 0.00608608666688\n",
        "RMS of derivative of noisy function: 0.211661774319\n"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x7358690>"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous code snippet, we show that the typical (RMS-sense) 1st order difference for a smooth function is much less than that of a non-smooth function. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Towards variational data assimilation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have seen above that the MAP (maximum *a posteriori*) estimate of the parameters is obtained by the maximum value of the posterior, or likewise, the minimum value of minus the log posterior. We have also seen how the likelihood and prior terms look like. Let's put them all together for a univeriate time series, $\\mathbf{x}$\n",
      "\n",
      "$$\n",
      "J_{obs} = -\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}_o\\right)^{T} \\mathbf{C_o}^{-1} \\left( \\mathbf{x} - \\mathbf{x}_o\\right)\n",
      "$$\n",
      "\n",
      "$$\n",
      "J_{prior} = -\\frac{1}{2}\\left(\\mathbf{x} - \\mathbf{x}_{p}\\right)^{T}\\mathbf{C_p}^{-1}\\left(\\mathbf{x} - \\mathbf{x}_p\\right) \n",
      "$$\n",
      "\n",
      "$$\n",
      "J_{model} = -\\frac{1}{2}\\left(\\Delta\\mathbf{x}\\right)^{T}\\mathbf{C_m}^{-1}\\left(\\Delta\\mathbf{x}\\right) \n",
      "$$\n",
      "\n",
      "The posterior is made up of the sum of the previous three terms:\n",
      "$$\n",
      "J = J_{obs} + J_{prior} + J_{model},\n",
      "$$\n",
      "and the MAP estimate will then just be minimum value of $-J$. We can solve for this efficiently using gradient descent methods, that use the derivative to locate the minimum value of the function, and are implemented in a number of computer languages and software packages. For that we can either calculate the derivatives analytically, or approximate them numerically. The former is a better approach, and can either be done \"by hand\", or by using automatic differentiation tools. We shall use approximations in this example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next code snippet, we introduce an implementation of the above cost function, called `cost_function`. The function takes as its first input $x$, and the other inputs are the other terms defined above. It returns the value of $J$ above. \n",
      "\n",
      "In the same snippet, we shall also create some synthetic data to play with. `y_smooth` is the ideal true function, to which we add noise (with a standard deviation of 0.15) to create `y_noisy`. We also simulate missing observations by creating `obs_mask`, a random 0/1 array. The prior is set to 0.5 for all values of x with a standard deviation of 1 (this is an uninformative prior). $\\sigma_{obs}$ is set to the value of the noise figure we added, so in this case, 0.15."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.optimize\n",
      "def cost_function ( x, y, x_prior, sigma_obs, sigma_prior, gamma, obs_mask ):\n",
      "    \"\"\"\n",
      "    Variational cost function with a weak constraint\n",
      "\n",
      "    Parameters\n",
      "    -----------\n",
      "    x: The current estimate of the state\n",
      "    y: The observations vector\n",
      "    x_prior: The prior estimate of the state\n",
      "    sigma_obs: Observational uncertainty (standard deviation)\n",
      "    sigma_prior: Prior uncertainty (standard deviation)\n",
      "    gamma: Inverse of the model term variance\n",
      "    \"\"\"\n",
      "    I = np.identity(x.shape[0])\n",
      "    D1 = np.matrix(I - np.roll(I,1))\n",
      "    D1a = D1*D1.T\n",
      "    j_obs = 0.5*np.sum((x[obs_mask]-y)**2/sigma_obs**2)\n",
      "    j_prior = np.sum((x-x_prior)**2/sigma_prior**2)\n",
      "    j_model = 0.5*gamma*(np.sum(np.array(D1a.dot(x))**2))\n",
      "    return j_obs + j_prior + j_model\n",
      "\n",
      "x = np.arange(1,366, 5 )\n",
      "y_smooth = (1-np.cos(np.pi*x/366.)**2)\n",
      "y_noisy = y_smooth + np.random.randn(x.shape[0])*0.15\n",
      "obs_mask = np.random.rand(x.shape[0])\n",
      "obs_mask = np.where ( obs_mask>0.7, True, False )\n",
      "x_prior = np.ones_like(x)*0.5\n",
      "sigma_prior = 1.0\n",
      "gamma = 100\n",
      "sigma_obs = 0.15\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we want to solve the problem. We use `scipy.optimize.fming_bfgs` to minimise `cost_function`. We start from the prior estimates. The loop sweeps over the value of gamma, to demonstrate its effect:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x[obs_mask],y_noisy[obs_mask], '-sg', lw=0.5)\n",
      "for gamma in np.logspace(0,4,5):\n",
      "    retval = scipy.optimize.fmin_bfgs ( cost_function, x_prior, args=(y_noisy[obs_mask], x_prior, sigma_obs, sigma_prior, gamma, obs_mask), disp=1, retall=1, fprime=None)\n",
      "    plt.plot(x,retval[0], '-',lw=1.4, label=\"%4g\"%gamma)\n",
      "plt.plot ( x, x_prior, '-k', label=\"Prior\")\n",
      "plt.legend(loc='best')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 8.041642\n",
        "         Iterations: 58\n",
        "         Function evaluations: 6000\n",
        "         Gradient evaluations: 80\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 12.149804\n",
        "         Iterations: 81\n",
        "         Function evaluations: 7125\n",
        "         Gradient evaluations: 95\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 16.319708\n",
        "         Iterations: 81\n",
        "         Function evaluations: 6600\n",
        "         Gradient evaluations: 88\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 19.117748\n",
        "         Iterations: 79\n",
        "         Function evaluations: 6450\n",
        "         Gradient evaluations: 86\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 22.851895\n",
        "         Iterations: 78\n",
        "         Function evaluations: 6300\n",
        "         Gradient evaluations: 84\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 219,
       "text": [
        "<matplotlib.legend.Legend at 0x21414e10>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x21414d10>"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous plot, look at the effect of $\\gamma$: For low $\\gamma$ (eg blue and green lines), we have a very good fit to the observations (green squares), but then the estimates drift to the original prior, as (i) the observations don't constrain them, and (ii) the smoothness term is very small. As $\\gamma$ becomes more important, we have a gradually smoother solution, that starts departing from the observed points, as the optimizer is balancing fit to the observations with fit to the \"process model\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see how the effect of noise in the observations (ie information content in the observations) affects the inference problem:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(1,366, 5 )\n",
      "y_smooth = (1-np.cos(np.pi*x/366.)**2)\n",
      "y_noisy1 = y_smooth + np.random.randn(x.shape[0])*0.15\n",
      "y_noisy2 = y_smooth + np.random.randn(x.shape[0])*0.5\n",
      "obs_mask1 = np.random.rand(x.shape[0])\n",
      "obs_mask1 = np.where ( obs_mask1>0.7, True, False )\n",
      "obs_mask2 = np.random.rand(x.shape[0])\n",
      "obs_mask2 = np.where ( obs_mask2>0.7, True, False )\n",
      "\n",
      "x_prior = np.ones_like(x)*0.5\n",
      "sigma_prior = 1.0\n",
      "gamma = 100\n",
      "\n",
      "plt.subplot(2,1,1)\n",
      "sigma_obs = 0.15\n",
      "plt.plot(x[obs_mask],y_noisy[obs_mask], '-sg', lw=0.5)\n",
      "for gamma in np.logspace(0,4,5):\n",
      "    retval = scipy.optimize.fmin_bfgs ( cost_function, x_prior, args=(y_noisy1[obs_mask1], x_prior, sigma_obs, sigma_prior, gamma, obs_mask1), disp=1, retall=1, fprime=None)\n",
      "    plt.plot(x,retval[0], '-',lw=1.4, label=\"%4g\"%gamma)\n",
      "plt.plot ( x, x_prior, '-k', label=\"Prior\")\n",
      "plt.subplot(2,1,2)\n",
      "sigma_obs = 0.5\n",
      "plt.plot(x[obs_mask],y_noisy[obs_mask], '-sg', lw=0.5)\n",
      "for gamma in np.logspace(0,4,5):\n",
      "    retval = scipy.optimize.fmin_bfgs ( cost_function, x_prior, args=(y_noisy2[obs_mask2], x_prior, sigma_obs, sigma_prior, gamma, obs_mask2), disp=1, retall=1, fprime=None)\n",
      "    plt.plot(x,retval[0], '-',lw=1.4, label=\"%4g\"%gamma)\n",
      "plt.plot ( x, x_prior, '-k', label=\"Prior\")\n",
      "\n",
      "#plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 4.822780\n",
        "         Iterations: 46\n",
        "         Function evaluations: 5475\n",
        "         Gradient evaluations: 73\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 8.317835\n",
        "         Iterations: 78\n",
        "         Function evaluations: 6750\n",
        "         Gradient evaluations: 90\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 11.547051\n",
        "         Iterations: 82\n",
        "         Function evaluations: 6900\n",
        "         Gradient evaluations: 92\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 12.961914\n",
        "         Iterations: 79\n",
        "         Function evaluations: 6375\n",
        "         Gradient evaluations: 85\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 15.091475\n",
        "         Iterations: 79\n",
        "         Function evaluations: 6225\n",
        "         Gradient evaluations: 83\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 5.530367\n",
        "         Iterations: 19\n",
        "         Function evaluations: 2925\n",
        "         Gradient evaluations: 39\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 7.214208\n",
        "         Iterations: 70\n",
        "         Function evaluations: 6150\n",
        "         Gradient evaluations: 82\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 8.347651\n",
        "         Iterations: 76\n",
        "         Function evaluations: 6525\n",
        "         Gradient evaluations: 87\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 8.888214\n",
        "         Iterations: 91\n",
        "         Function evaluations: 26570\n",
        "         Gradient evaluations: 353\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 9.211992\n",
        "         Iterations: 146\n",
        "         Function evaluations: 108549\n",
        "         Gradient evaluations: 1439\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 220,
       "text": [
        "[<matplotlib.lines.Line2D at 0x1fb6bfd0>]"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x1f5e97d0>"
       ]
      }
     ],
     "prompt_number": 220
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also see the effect of having an informative prior, maybe from some climatology or something. We will assume that the prior mean is `y_smooth`, with a standard deviation of 0.3, and we will use the same noisy observations as the bottom panel in the previous plot:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma_prior = 0.4\n",
      "x_prior = y_smooth + np.random.randn ( y_smooth.shape[0] ) *0.1\n",
      "sigma_obs = 0.5\n",
      "plt.plot(x[obs_mask],y_noisy[obs_mask], '-sg', lw=0.5)\n",
      "for gamma in np.logspace(0,4,5):\n",
      "    retval = scipy.optimize.fmin_bfgs ( cost_function, x_prior*0 + 0.5, args=(y_noisy2[obs_mask2], x_prior, sigma_obs, sigma_prior, gamma, obs_mask2), disp=1, retall=1, fprime=None)\n",
      "    plt.plot(x,retval[0], '-',lw=1.4, label=\"%4g\"%gamma)\n",
      "plt.plot ( x, x_prior, '-k', label=\"Prior\")\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 10.433466\n",
        "         Iterations: 10\n",
        "         Function evaluations: 1575\n",
        "         Gradient evaluations: 21\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 12.365154\n",
        "         Iterations: 25\n",
        "         Function evaluations: 3825\n",
        "         Gradient evaluations: 51\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 13.419765\n",
        "         Iterations: 85\n",
        "         Function evaluations: 7650\n",
        "         Gradient evaluations: 102\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 14.325583\n",
        "         Iterations: 82\n",
        "         Function evaluations: 6825\n",
        "         Gradient evaluations: 91\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 16.922321\n",
        "         Iterations: 100\n",
        "         Function evaluations: 35342\n",
        "         Gradient evaluations: 469\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 221,
       "text": [
        "<matplotlib.legend.Legend at 0x1fa10b10>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x1fa10990>"
       ]
      }
     ],
     "prompt_number": 221
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The optimisation routine works by minimising `cost_function`, but it approximates the partial derivatives of `cost_function` with respect to `x` numerically. These derivatives are easy to calculate, so the exercise is to write some code that calculates them. \n",
      "\n",
      "**Hint**\n",
      "You can find expressions in the [EO-LDAS paper](http://www.sciencedirect.com/science/article/pii/S0034425712000788)!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The derivatives..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def der_cost_function ( x, y, x_prior, sigma_obs, sigma_prior, gamma, obs_mask ):\n",
      "    \"\"\"This function calculates the partial derivatives of the cost function\"\"\"\n",
      "    I = np.identity(x.shape[0])\n",
      "    D1 = np.matrix(I - np.roll(I,1))\n",
      "    D1a = D1*D1.T\n",
      "    \n",
      "    der_j_obs = x*0\n",
      "    der_j_obs[obs_mask] = (x[obs_mask] - y)/sigma_obs**2\n",
      "    der_j_prior = (x - x_prior ) /sigma_prior**2\n",
      "    der_j_model = np.array(gamma*np.dot((D1).T, D1*np.matrix(x).T)).squeeze()\n",
      "    return der_j_obs + der_j_prior + der_j_model\n",
      "\n",
      "def cost_function ( x, y, x_prior, sigma_obs, sigma_prior, gamma, obs_mask ):\n",
      "    \"\"\"\n",
      "    Variational cost function with a weak constraint\n",
      "\n",
      "    Parameters\n",
      "    -----------\n",
      "    x: The current estimate of the state\n",
      "    y: The observations vector\n",
      "    x_prior: The prior estimate of the state\n",
      "    sigma_obs: Observational uncertainty (standard deviation)\n",
      "    sigma_prior: Prior uncertainty (standard deviation)\n",
      "    gamma: Inverse of the model term variance\n",
      "    \"\"\"\n",
      "    I = np.identity(x.shape[0])\n",
      "    D1 = np.matrix(I - np.roll(I,1))\n",
      "    D1a = D1*D1.T\n",
      "    xa = np.matrix ( x )\n",
      "    j_obs = 0.5*np.sum((x[obs_mask]-y)**2/sigma_obs**2)\n",
      "    j_prior = 0.5*np.sum((x-x_prior)**2/sigma_prior**2)\n",
      "    j_model = 0.5*gamma*np.dot((D1*(xa.T)).T, D1*xa.T)\n",
      "    return j_obs + j_prior + j_model\n",
      "\n",
      "\n",
      "x = np.arange(1,366)\n",
      "y_smooth = (1-np.cos(np.pi*x/366.)**2)\n",
      "y_noisy = y_smooth + np.random.randn(x.shape[0])*0.15\n",
      "obs_mask = np.random.rand(x.shape[0])\n",
      "obs_mask = np.where ( obs_mask>0.7, True, False )\n",
      "x_prior = np.ones_like(x)*0.5\n",
      "sigma_prior = 1.0\n",
      "gamma = 100\n",
      "sigma_obs = 0.15\n",
      "\n",
      "\n",
      "plt.plot(x[obs_mask],y_noisy[obs_mask], '-sg', lw=0.5)\n",
      "for gamma in np.logspace(0,4,5):\n",
      "    retval = scipy.optimize.fmin_bfgs ( cost_function, x_prior, fprime=der_cost_function,\\\n",
      "    args=(y_noisy[obs_mask], x_prior, sigma_obs, sigma_prior, gamma, obs_mask), disp=1, retall=1)\n",
      "    plt.plot(x,retval[0], '-',lw=1.4, label=\"%4g\"%gamma)\n",
      "plt.plot ( x, x_prior, '-k', label=\"Prior\")\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 15.955734\n",
        "         Iterations: 52\n",
        "         Function evaluations: 75\n",
        "         Gradient evaluations: 75\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 33.061585\n",
        "         Iterations: 98\n",
        "         Function evaluations: 108\n",
        "         Gradient evaluations: 108\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 61.311119\n",
        "         Iterations: 179\n",
        "         Function evaluations: 181\n",
        "         Gradient evaluations: 181\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 84.606545\n",
        "         Iterations: 374\n",
        "         Function evaluations: 381\n",
        "         Gradient evaluations: 381\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 139.970998\n",
        "         Iterations: 370\n",
        "         Function evaluations: 378\n",
        "         Gradient evaluations: 378\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 222,
       "text": [
        "<matplotlib.legend.Legend at 0x2192c290>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x2192c310>"
       ]
      }
     ],
     "prompt_number": 222
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous test, we see that now the number of function and gradient evaluations is similar. While the evaluation of the function and its gradient for this problem is fairly fast and using numerical approximations is reasonable, note that in more complex and higher dimensional problem, each gradient evaluation needs many evaluations of the original cost function!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Uncertainty"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous discussion, the estimation of the full uncertainty associated with our inference on the state has been missing: we have just been concerned with the calculation of the **posterior mode**, and have paid no attention to the full posterior distribution. It turns out that if everything is Gaussian and linear (like the above problems), then the posterior distribution is also Gaussian, with the mean that is the posterior model we have calculated above, and with a covariance matrix given by the inverse of the Hessian. Given that we have already calculated the derivatives of the cost function, it's easy to calculate the Hessian (**do it!!!**)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "I = np.identity(x.shape[0])\n",
      "D1 = np.matrix(I - np.roll(I,1))\n",
      "\n",
      "hess_obs = np.zeros( x.shape[0] )\n",
      "hess_obs[obs_mask] = (1./sigma_obs**2)\n",
      "hess_obs = np.diag( hess_obs )\n",
      "\n",
      "hess_prior = (1./sigma_prior**2)*np.diag(np.ones_like(x))\n",
      "\n",
      "hess_model = gamma*np.dot ( D1,np.eye(x.shape[0])).dot( D1.T)\n",
      "hess = hess_obs + hess_prior + hess_model\n",
      "post_covar = np.linalg.inv ( hess )\n",
      "plt.subplot(1,3,1)\n",
      "plt.imshow ( hess_prior, interpolation='nearest')\n",
      "plt.yticks(visible=False)\n",
      "plt.xticks(visible=False)\n",
      "plt.title(\"Prior contribution to the Hessian\")\n",
      "plt.subplot(1,3,2)\n",
      "plt.imshow ( hess_obs, interpolation='nearest')\n",
      "plt.yticks(visible=False)\n",
      "plt.xticks(visible=False)\n",
      "plt.title(\"Obs contribution to the Hessian\")\n",
      "plt.subplot(1,3,3)\n",
      "plt.imshow ( hess_model, interpolation='nearest')\n",
      "plt.yticks(visible=False)\n",
      "plt.xticks(visible=False)\n",
      "plt.title(\"Model contribution to the Hessian\")\n",
      "plt.figure()\n",
      "plt.imshow ( post_covar, interpolation='nearest', cmap=plt.cm.spectral)\n",
      "plt.yticks(visible=False)\n",
      "plt.xticks(visible=False)\n",
      "plt.title(\"Posterior covariance Matrix\")\n",
      "plt.colorbar()\n",
      "\n",
      "D = np.sqrt(np.diagonal(post_covar))\n",
      "Dinv = np.linalg.inv( np.diag(D) ) # Unnecessary, it's diagonal...\n",
      "post_covar = np.matrix(post_covar)\n",
      "Dinv = np.matrix(Dinv)\n",
      "corr = Dinv * post_covar * Dinv\n",
      "plt.figure()\n",
      "plt.imshow ( corr, interpolation='nearest', cmap=plt.cm.RdBu_r, vmin=-1, vmax=1)\n",
      "plt.yticks(visible=False)\n",
      "plt.xticks(visible=False)\n",
      "plt.colorbar()\n",
      "plt.title(\"Posterior Correlation matrix\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 223,
       "text": [
        "<matplotlib.text.Text at 0x255d90d0>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x219304d0>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x2513f050>"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x255c4e90>"
       ]
      }
     ],
     "prompt_number": 223
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Putting it together in the above experiments..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def posterior_covariance (x, sigma_prior, sigma_obs, gamma):\n",
      "    I = np.identity(x.shape[0])\n",
      "    D1 = np.matrix(I - np.roll(I,1))\n",
      "\n",
      "    hess_obs = np.zeros( x.shape[0] )\n",
      "    hess_obs[obs_mask] = (1./sigma_obs**2)\n",
      "    hess_obs = np.diag( hess_obs )\n",
      "    hess_prior = (1./sigma_prior**2)*np.diag(np.ones_like(x))\n",
      "\n",
      "    hess_model = gamma*np.dot ( D1,np.eye(x.shape[0])).dot( D1.T)\n",
      "    hess = hess_obs + hess_prior + hess_model\n",
      "    post_covar = np.linalg.inv ( hess )\n",
      "    return np.sqrt(np.diagonal(post_covar))\n",
      "\n",
      "\n",
      "x = np.arange(1,366)\n",
      "y_smooth = (1-np.cos(np.pi*x/366.)**2)\n",
      "y_noisy = y_smooth + np.random.randn(x.shape[0])*0.15\n",
      "\n",
      "obs_mask = np.random.rand(x.shape[0])\n",
      "obs_mask = np.where ( obs_mask>0.7, True, False )\n",
      "obs1 = np.zeros_like(x, dtype=np.bool)\n",
      "obs1[::5] = obs_mask[::5]\n",
      "obs_mask = obs1\n",
      "\n",
      "x_prior = np.ones_like(x)*0.5\n",
      "sigma_prior = 1.0\n",
      "gamma = 100\n",
      "sigma_obs = 0.15\n",
      "\n",
      "\n",
      "iplot=1\n",
      "for gamma in np.logspace(0,6,10):\n",
      "    plt.subplot(5,2,iplot)\n",
      "    iplot = iplot+1\n",
      "    plt.plot(x[obs_mask],y_noisy[obs_mask], '-+', ms=5)\n",
      "    retval = scipy.optimize.fmin_bfgs ( cost_function, x_prior, fprime=der_cost_function,\\\n",
      "        args=(y_noisy[obs_mask], x_prior, sigma_obs, sigma_prior, gamma, obs_mask), \\\n",
      "        disp=1, retall=1)\n",
      "    post_sd = posterior_covariance ( retval[0], sigma_prior, sigma_obs, gamma )\n",
      "    plt.fill_between(x, retval[0] - 1.96*post_sd, retval[0] + 1.96*post_sd, color='0.8' )\n",
      "    plt.plot(x,retval[0], '-r',lw=1.4 )\n",
      "    plt.yticks(visible=False)\n",
      "    plt.xticks(visible=False)\n",
      "    plt.ylim(-0.3, 1.2)\n",
      "    plt.xlim(0, 400)\n",
      "    plt.text(275,0.92,r'$\\gamma=%4.2G$'%gamma, size=\"smaller\")\n",
      "    \n",
      "fig = plt.gcf()\n",
      "fig.suptitle('#Obs/year: %d\\nNoise: %4.2G'% ( obs_mask, sigma_obs ).sum() )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 4.545885\n",
        "         Iterations: 17\n",
        "         Function evaluations: 32\n",
        "         Gradient evaluations: 32\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 8.448397\n",
        "         Iterations: 46\n",
        "         Function evaluations: 65\n",
        "         Gradient evaluations: 65\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 14.587063\n",
        "         Iterations: 103\n",
        "         Function evaluations: 107\n",
        "         Gradient evaluations: 107\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 21.362238\n",
        "         Iterations: 180\n",
        "         Function evaluations: 183\n",
        "         Gradient evaluations: 183\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 28.802163\n",
        "         Iterations: 367\n",
        "         Function evaluations: 374\n",
        "         Gradient evaluations: 374\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 40.872956\n",
        "         Iterations: 371\n",
        "         Function evaluations: 382\n",
        "         Gradient evaluations: 382\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 72.193094\n",
        "         Iterations: 372\n",
        "         Function evaluations: 388\n",
        "         Gradient evaluations: 388\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 145.017488\n",
        "         Iterations: 371\n",
        "         Function evaluations: 387\n",
        "         Gradient evaluations: 387\n",
        "Optimization terminated successfully."
       ]
      },
      {
       "ename": "IndexError",
       "evalue": "index 10 is out of bounds for axis 0 with size 1",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-224-168334290401>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     retval = scipy.optimize.fmin_bfgs ( cost_function, x_prior, fprime=der_cost_function,\\\n\u001b[0;32m     38\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_noisy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_prior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_prior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         disp=1, retall=1)\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpost_sd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposterior_covariance\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_prior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_between\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.96\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpost_sd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.96\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpost_sd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.8'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfmin_bfgs\u001b[1;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[0;32m    718\u001b[0m             'return_all': retall}\n\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m                      line_search_wolfe2(f, myfprime, xk, pk, gfk,\n\u001b[1;32m--> 799\u001b[1;33m                                         old_fval, old_old_fval)\n\u001b[0m\u001b[0;32m    800\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0malpha_k\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m                 \u001b[1;31m# This line search also failed to find a better solution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/optimize/linesearch.pyc\u001b[0m in \u001b[0;36mline_search_wolfe2\u001b[1;34m(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[0malpha_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi_star\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                 scalar_search_wolfe2(phi, derphi, old_fval, old_old_fval,\n\u001b[1;32m--> 256\u001b[1;33m                                      derphi0, c1, c2, amax)\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mderphi_star\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/optimize/linesearch.pyc\u001b[0m in \u001b[0;36mscalar_search_wolfe2\u001b[1;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax)\u001b[0m\n\u001b[0;32m    354\u001b[0m                         _zoom(alpha0, alpha1, phi_a0,\n\u001b[0;32m    355\u001b[0m                               \u001b[0mphi_a1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi_a0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                               phi0, derphi0, c1, c2)\n\u001b[0m\u001b[0;32m    357\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/optimize/linesearch.pyc\u001b[0m in \u001b[0;36m_zoom\u001b[1;34m(a_lo, a_hi, phi_lo, phi_hi, derphi_lo, phi, derphi, phi0, derphi0, c1, c2)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;31m# Check new value of a_j\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m         \u001b[0mphi_aj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mphi_aj\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mphi0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma_j\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mderphi0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mphi_aj\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mphi_lo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[0mphi_rec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphi_hi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/optimize/linesearch.pyc\u001b[0m in \u001b[0;36mphi\u001b[1;34m(alpha)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mfc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-222-c1ba468143b6>\u001b[0m in \u001b[0;36mcost_function\u001b[1;34m(x, y, x_prior, sigma_obs, sigma_prior, gamma, obs_mask)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mD1a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mD1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mxa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mj_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msigma_obs\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mj_prior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msigma_prior\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mj_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 221.277347\n",
        "         Iterations: 370\n",
        "         Function evaluations: 386\n",
        "         Gradient evaluations: 386\n"
       ]
      },
      {
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x255f2d50>"
       ]
      }
     ],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x[obs_mask],y_noisy[obs_mask], '-+', ms=5)\n",
      "\n",
      "plt.fill_between(x, retval[0] - 1.96*post_sd, retval[0] + 1.96*post_sd, color='0.8' )\n",
      "plt.plot(x,retval[0], '-r',lw=1.4 )\n",
      "plt.plot ( x, x_prior, '-k', label=\"Prior\")\n",
      "plt.yticks([0,1])\n",
      "plt.xticks(visible=False)\n",
      "plt.ylim(-0.3, 1.2)\n",
      "plt.xlim(0, 400)\n",
      "plt.text(275,0.92,r'$\\gamma=%4.2G$'%gamma, size=\"smaller\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}